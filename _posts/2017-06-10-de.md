---
layout:     post
title:      'Evolutionary ALgorithms I: Differential Evolution'
date:       2017-06-11 09:10:00
summary:    Differential Evolution algorithm and some experiments
categories: blog ea de
---

Evolutionary Algorithms are classified under a family of algorithms for global optimization by biological evolution, and are based on meta-heuristic search approaches. The possible solutions for usually span a n-dimensional vector space over the problem domain and we simulate several population particles to reach a global optimum.  

An optimization problem, in a basic form, consists of solving the task of maximizing or minimizing a real function by choosing values from a pool of possible solution elements (vectors) according to procedural instructions provided for the algorithm. Evolutionary approaches usually follow a specific strategy with differenet variations to select candidate elements from population set and apply crossover and/or mutations to modify the elements while trying to improve the quality of modified elements.

These algorithms can be applied to several interesting applications as well, and have been shown to perform very well in optimizing NP-hard problems as well, including the Travelling Salesman Problem, Job-Shop Scheduling, Graph coloring while also having applicaitons in domains such as Signals and Systems, Mechanical Engineering, and solving mathematical optimization problems.

One such algorithm belonging to the family of Evolutionary Algorithms is Differential Evolution (DE) algorithm. In this post, we shall be discussing about a few properties of the Diferential Evolution algorithm while implementing it in Python (github link) for optimizing a few test functions.

## Differential Evolution

DE approaches an optimization problem iteratively trying to improve a set of candidate solutions for a given measure of quality (cost function). These set of algorithms fall under meta-heuristics since they make few or no assumptions about the problem being optimized and can search very large spaces of possible solution elements. The algorithm involves maintaining a population of candidate solutions subjected to iterations of recombination, evaluation and selection. The creation of new candidate solution requires the application of a linear operation on selected elements using a parameter $$F$$ called differential weight from population to generate a vector element and then randomly applying crossover based on the parameter Crossover Probability. $$CR$$.

The algorithm follows the steps listed down:

1. Initialize a set of agents/elements $$x$$ with random positions in the search space for population size $$P$$.
2. Until a termination criterion is met (number of iterations or required optimality), repeat the following for each agent $$x_i$$:
	* Pick three agents $$a, b$$, and $$c$$ from the population at random (distnct).
	* Pick a random index $$R \in \{1,...,n\}$$ ($$n$$ is the dimensionality of the problem)
	* Compute a temporary vector $$y$$ as following:

		$$y = a + F (b-c)$$
	* Now, for each $$j \in \{1,...,n\}$$, pick a uniformly distributed number $$r_i \equiv U(0, 1)$$.
	* If $$r_i \lt CR$$ or $$i=R$$, then
		+ set $$x_{I, j} = y_{j}$$
	* Otherwise, $$x_{I, j} = x_{i, j}$$

	* if $$f(x_{I}) \lt f(x_i)$$, ($$f$$ is the cost function for minimization), then
		+ replace $$x_i$$ with $$x_i$$.
	* otherwise, $$x_i$$ remains unchanged.
3. Pick the agent from the population that has the highest fitness or lowest cost function value as the solution.

## Implementing the Algorithm

The directory structure for the code follows the design as given below:

{% highlight python %}
.
├── differential_evolution.py
└── helpers
    ├── __init__.py
    ├── collection.py
    ├── point.py
    └── test_functions.py
{% endhighlight %}

Where, differential_evolution.py is the main file we'll run for execution of the algorithm.
The helpers directory consists of helper classes and functions for several operations such as handling the point objects and vector operations related to candidate elements (point.py), methods for handling the collection of all such points and building the population (collection.py), test functions to be used objective/cost functions for testing the efficiency of the algorithm (test_functions.py).

### Building Point Class

{% highlight python lineanchors %}
# helpers/point.py

import numpy as np
import scipy as sp

from test_functions import evaluate


class Point:
    def __init__(self, dim=2, upper_limit=10, lower_limit=-10):
        self.dim = dim
        self.coords = np.zeros((self.dim,))
        self.z = None
        self.range_upper_limit = upper_limit
        self.range_lower_limit = lower_limit
        self.evaluate_point()

    def generate_random_point(self):
        self.coords = np.random.uniform(self.range_lower_limit, self.range_upper_limit, (self.dim,))
        self.evaluate_point()

    def evaluate_point(self):
        self.z = evaluate(self.coords)

{% endhighlight %}

--
 